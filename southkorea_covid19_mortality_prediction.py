# -*- coding: utf-8 -*-
"""southkorea_covid19_mortality_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bEs5mUA_rhFL0xNnm78K1f4px1TsSCbp
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn import under_sampling, over_sampling
from imblearn.over_sampling import SMOTE
from scipy.optimize import fmin_tnc # compute the minimum for function
from sklearn import svm
from sklearn import metrics
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
import itertools
#pip install imblearn
#pip install spicy

"""1. Import dataset


"""

xls = pd.ExcelFile('patient.xlsx')

original_data = pd.read_excel(xls, 'original')
original_data.shape
original_data

"""2. Descriptive analysis

Performed descriptive analyses of the predictors by respective stratification groups and present the results as numbers 
"""

gp1=original_data.groupby(by=['sex'])
gp1.size()

gp2=original_data.groupby(by=['age'])
gp2.size()

gp3=original_data.groupby(by=['province'])
gp3.size()

gp4=original_data.groupby(by=['infection_case'])
gp4.size()

gp5=original_data.groupby(by=['state'])
gp5.size()

"""3. Data prepocessing

As the infection_case variable has too many types, so we wanna regroup it to fewer categories. Specifically, we followed prior study to recategory it to 9 groups, which are: contact with patient,overseas inflow,etc,Nursing home,Hospital ,Religious gathering, Call center, CSA(Community center, shelter and apartment),Gym facility. Also, since our goal is to perdict the mortality,we need to catergorize both the released and isolated cases to survived group. Then we cleaned cases that have missing values.

4. Import prepocessed data
"""

data = pd.read_excel(xls, 'DP(without missing)') 
data = data.drop('patient_id', 1) 
data

data.shape

sns.countplot(x = "sex", data=data) plt.show() 
sns.countplot(x = "age", data=data) 
plt.show()

sns.countplot(x = "province", data=data) plt.show() 
sns.countplot(x = "infection_case", data=data) plt.show() 
sns.countplot(x = "state", data=data) 
plt.show()

"""5. Encode categorical data

Our data contains categorical data, so we must encode it to numbers before we can fit and evaluate the model.

"""

data['sex'].replace(['female','male'],[0,1],inplace=True) data['state'].replace(['deceased','Survived'],[0,1],inplace=True) 
data['age'].replace(['0s','10s','20s','30s','40s','50s','60s','70s' ,'80s','90s','100s'],[1,2,3,4,5,6,7,8,9,10,11],inplace=True) 
# using one hot encoding approach to encode categorical features data = pd.get_dummies(data, columns=["province"]) 
data = pd.get_dummies(data, columns=["infection_case"]) 
# create intercept 
data['intercept'] = [1] * data.iloc[:,:].shape[0] 
# move depedendent variable to last column 
# move intercept to first column 
data = data[[ col for col in data.columns if col != 'state' ] + ['s tate']] 
data = data[['intercept'] + [ col for col in data.columns if col != 'intercept' ]] 
data

type(data.iloc[0,30])

"""6. Logistic regression model 

6.1 Logistic regression method 1 

In method 1, we define the model by ourselves. We use sigmoid function as activation function, and set threshold to 0.5. 

"""

# retrieve numpy array 
dataset = data.values 
# split into input (X) and output (y) variables 
X = dataset[:, :-1] 
y = dataset[:,-1] 
# reshape target to be a 2d array 
y = y.reshape((len(y), 1)) 
# split into train and test sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=1) 
# summarize 
print('Train', X_train.shape, y_train.shape) 
print('Test', X_test.shape, y_test.shape) 
theta = np.zeros((X_train.shape[1], 1))

def sigmoid(x): 
  # Activation function used to map any real value between 0 and 1 
  return 1 / (1 + np.exp(-x)) 
def net_input(theta, x): 
  # Computes the weighted sum of inputs 
  return np.dot(x, theta) 
def probability(theta, x): 
  # Returns the probability after passing through sigmoid  
  return sigmoid(net_input(theta, x)) 
def cost_function(theta, x, y): 
  # Computes the cost function for all the training samples  m = x.shape[0] 
  total_cost = -(1 / m) * np.sum( y * np.log(probability(theta, x)) + (1 - y) * np.log(  1 - probability(theta, x))) 
  return total_cost 
def gradient(theta, x, y): 
  # Computes the gradient of the cost function at the point theta  m = x.shape[0] 
  return (1 / m) * np.dot(x.T, sigmoid(net_input(theta, x)) - y ) 
def fit(x, y, theta):
  opt_weights = fmin_tnc(func=cost_function, x0=theta,  fprime=gradient,args=(x, y.flatten()))  
  return opt_weights[0] 
def predict(x): 
  theta = parameters[:, np.newaxis] 
  return probability(theta, x) 
def accuracy(x, actual_classes, probab_threshold=0.5):  
  predicted_classes = (predict(x) >= probab_threshold).astype(int)  
  predicted_classes = predicted_classes.flatten()  
  accuracy_score = np.mean(predicted_classes == actual_classes)  
  return accuracy_score 
def auc(x, actual_classes, probab_threshold=0.5): 
  predicted_classes = (predict(x) >= 
  probab_threshold).astype(int)  
  predicted_classes = predicted_classes.flatten()  
  auc_score = metrics.roc_auc_score(actual_classes.flatten(),pred icted_classes) 
  return auc_score 
def precision(x, actual_classes, probab_threshold=0.5):  
  predicted_classes = (predict(x) >= probab_threshold).astype(int)  
  predicted_classes = predicted_classes.flatten() 
  precision_score = metrics.precision_score(actual_classes.flatte n(),predicted_classes) 
  return precision_score 
def recall(x, actual_classes, probab_threshold=0.5):  
  predicted_classes = (predict(x) >=  probab_threshold).astype(int)  
  predicted_classes = predicted_classes.flatten()  
  recall_score = metrics.recall_score(actual_classes.flatten(),pr edicted_classes) 
  return recall_score 
def class_report(x, actual_classes, probab_threshold=0.5):  
  predicted_classes = (predict(x) >= probab_threshold).astype(int)   
  predicted_classes = predicted_classes.flatten()  
  print(classification_report(actual_classes.flatten(), predicted _classes)) 
  return
def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=False): 
  accuracy = np.trace(cm) / np.sum(cm).astype('float')  
  misclass = 1 - accuracy 
  if cmap is None: 
    cmap = plt.get_cmap('Blues') 
  plt.figure(figsize=(8, 6)) 
  plt.imshow(cm, interpolation='nearest', cmap=cmap)  
  plt.title(title) 

  if target_names is not None: 
    tick_marks = np.arange(len(target_names)) 
 plt.xticks(tick_marks, target_names, rotation=45)  plt.yticks(tick_marks, target_names) 
  if normalize: 
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] 
  thresh = cm.max() / 1.5 
  if normalize else cm.max() / 2  
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shap e[1])): 
    if normalize: 
      plt.text(j, i, "{:0.4f}".format(cm[i, j]),  horizontalalignment="center", color="white" if cm[i, j] > thresh else "black ") 
    else:
      plt.text(j, i, "{:,}".format(cm[i, j]),  horizontalalignment="center", color="white" if cm[i, j] > thresh else "black ") 
      plt.tight_layout() 
      plt.ylabel('True label') 
      plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f} '.format(accuracy, misclass)) 
      plt.show() 
  
def confusion_matrix_method_1(x, actual_classes, probab_threshold=0 .5): 
  predicted_classes = (predict(x) >= probab_threshold).astype(int)  
  predicted_classes = predicted_classes.flatten()  
  plot_confusion_matrix(confusion_matrix(actual_classes.flatten() 
  predicted_classes), ['Deceased', 'Survived']) 
  # fit the training data 
  parameters = fit(X_train, y_train, theta) 
   # calculate accurancy result 
  accuracy_6_1 = accuracy(X_test, y_test) 
  auc_6_1 = auc(X_test, y_test) 
  precision_6_1 = precision(X_test, y_test) 
  rrecall_6_1 = recall(X_test, y_test) 
  print('Accuracy:', accuracy_6_1) 
  print('Auc:', auc_6_1) 
  print('Precision:', precision_6_1) 
  print('Recall:', recall_6_1) 
  #class_report(X_test, y_test) 
  confusion_matrix_method_1(X_test, y_test)

"""We get a revelative low AUC acore compared with other matrics. 
From the confusion matrix we can see that we have 21 worng cases. There are 17 decreased cases but we predicte as survived and 4 survived cases but we predict as decreased.

6.2 Logistic regression method 2 

implement classifier using scikit-learn
"""

model = LogisticRegression() 
model.fit(X_train, y_train) 
predicted_classes = model.predict(X_test) 
#parameters = model.coef_ 
accuracy_6_2 = metrics.accuracy_score(y_test,predicted_classes) auc_6_2 = metrics.roc_auc_score(y_test,predicted_classes) 
precision_6_2 = metrics.precision_score(y_test.flatten(),predicted_ classes) 
recall_6_2 = metrics.recall_score(y_test.flatten(),predicted_classe s) 
print('Accuracy:', accuracy_6_2) 
print('Auc:', auc_6_2) 
print('Precision:', precision_6_2) 
print('Recall:', recall_6_2) 
#print(classification_report(y_test, predicted_classes)) 
plot_confusion_matrix(confusion_matrix(y_test, predicted_classes), ['Deceased', 'Survived'])

"""The AUC score is also low. This might be caused by imbalanced data. So we will deal with imbalanced data in the following dection. 
From the confusion matrix we can see that we have 17 worng cases. There are 17 decreased cases but we predicte as survived

6.3 Logistic regression

Using SMOTE to deal with the imbalanced data

As our data was imbalanced, we applied one oversampling technique called synthetic minority oversampling technique (SMOTE) to enhance the learning on the training data (Chawla et al.,2002; Nnamoko & Korkontzelos, 2020). SMOTE creates synthetic samples from the minority class (cases with deaths in our data) according to feature space similarities between nearest neighbors.
"""

#Using SMOTE to deal with the imbalanced data 
x_resampled,y_resampled = SMOTE().fit_sample(X,y) 
X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(x_r esampled, y_resampled, test_size=0.2, random_state=1)

# fit Logistic regression method 1 with balanced data parameters = fit(X_train_re, y_train_re, theta) 
accuracy_6_3_1 = accuracy(X_test_re, y_test_re) 
auc_6_3_1 = auc(X_test_re, y_test_re) 
precision_6_3_1 = precision(X_test_re, y_test_re) 
recall_6_3_1 = recall(X_test_re, y_test_re) 
print('#######Logistic regression method 1 with balanced dataset:## #####') 
print('Accuracy:', accuracy_6_3_1) 
print('Auc:', auc_6_3_1) 
print('Precision:', precision_6_3_1) 
print('Recall:', recall_6_3_1) 
#class_report(X_test_re, y_test_re) 
confusion_matrix_method_1(X_test_re, y_test_re) 
print('') 
print('') 
# fit Logistic regression method 2 with balanced data 
model.fit(X_train_re, y_train_re) 
predicted_classes_re = model.predict(X_test_re) 
accuracy_6_3_2 = metrics.accuracy_score(y_test_re,predicted_classes _re) 
auc_6_3_2 = metrics.roc_auc_score(y_test_re,predicted_classes_re) precision_6_3_2 = metrics.precision_score(y_test_re.flatten(),predi cted_classes_re) 
recall_6_3_2 = metrics.recall_score(y_test_re.flatten(),predicted_c lasses_re) 
print('#######Logistic regression method 2 with balanced dataset:## ######') 
print('Accuracy:', accuracy_6_3_2) 
print('Auc:', auc_6_3_2) 
print('Precision:', precision_6_3_2) 
print('Recall:', recall_6_3_2) 
#print(classification_report(y_test_re, predicted_classes_re)) plot_confusion_matrix(confusion_matrix(y_test_re, predicted_classes _re), ['Deceased', 'Survived'])

"""All four metrics performed well on our models. 
Method 1: From the confusion matrix we can see that we have 72 worng cases. There are 26 decreased cases but we predicte as survived and 46 survived cases but we predict as decreased. 
Method 2: From the confusion matrix we can see that we have 73 worng cases. There are 19 decreased cases but we predicte as survived and 54 survived cases but we predict as decreased.

7. SVM method

7.1 SVM Classifier 

Using linear kernal
"""

#Create a svm Classifier 
svm_linear = svm.SVC(kernel='linear') # Linear Kernel 
#Train the model using the training sets 
svm_linear.fit(X_train, y_train) 
#Predict the response for test dataset 
y_pred_linear = svm_linear.predict(X_test) 
# Model Accuracy: how often is the classifier correct? 
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_linear)) print("AUC:",metrics.roc_auc_score(y_test, y_pred_linear))

"""7.2 SVM Classifier 

Using poly kernal 

"""

#Create a svm Classifier using poly kernal  
svm_poly = svm.SVC(kernel='poly') 
svm_poly.fit(X_train, y_train) 
y_pred_poly = svm_poly.predict(X_test) 

print("Accuracy:",metrics.accuracy_score(y_test, y_pred_poly)) 
print("AUC:",metrics.roc_auc_score(y_test, y_pred_poly))

"""7.3 SVM Classifier using RBF kernal 

"""

#Create a svm Classifier using RBF kernal  
svm_rbf = svm.SVC(kernel='rbf') 
svm_rbf.fit(X_train, y_train) 
y_pred_rbf = svm_rbf.predict(X_test) 
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_rbf)) 
print("AUC:",metrics.roc_auc_score(y_test, y_pred_rbf))

"""7.4 SVM: using SMOTE to deal with the imbalanced data

If we don't deal with the imbalanced data, the AUC is close to 0.5, which is very low. So we also used SMOTE to try to enhance the learning on the training data. 
"""

svm_linear = svm.SVC(kernel='linear') # Linear Kernel svm_linear.fit(X_train_re, y_train_re) 
y_pred_linear_re = svm_linear.predict(X_test_re) 
accuracy_7_4 = metrics.accuracy_score(y_test_re,y_pred_linear_re) 
auc_7_4 = metrics.roc_auc_score(y_test_re,y_pred_linear_re) 
precision_7_4 = metrics.precision_score(y_test_re,y_pred_linear_re) 
recall_7_4 = metrics.recall_score(y_test_re,y_pred_linear_re) 
print('Accuracy:', accuracy_7_4) 
print('Auc:', auc_7_4) 
print('Precision:', precision_7_4) 
print('Recall:', recall_7_4) 
#print(classification_report(y_test_re, y_pred_linear_re)) 
plot_confusion_matrix(confusion_matrix(y_test_re, y_pred_linear_re) , ['Deceased', 'Survived'])

"""From the confusion matrix we can see that we have 73 worng cases. There are 16 decreased cases but we predicte as survived and 57 survived cases but we predict as decreased.

8. Visualize LR classifier

In order to better visualize the decision boundary, we will perform principal component analysis (PCA) on the data to reduce the dimensionality to 2 dimensions.
"""

X = data.iloc[:,:-1] 
y = data.iloc[:,-1] 
pca = PCA(n_components=2).fit_transform(X) 
X_train1, X_test1, y_train1, y_test1 = train_test_split(pca, y, ran dom_state=0) 

plt.figure(dpi=120) 
plt.scatter(pca[y.values==0,0], pca[y.values==0,1], alpha=0.5, labe l='YES', s=2, color='navy') 
plt.scatter(pca[y.values==1,0], pca[y.values==1,1], alpha=0.5, labe l='NO', s=2, color='darkorange') 
plt.legend() 
plt.title('Covid-19 dataset\nFirst Two Principal Components') 
plt.xlabel('PC1') 
plt.ylabel('PC2') 
plt.gca().set_aspect('equal') 
plt.show()

def plot_bank(X, y, fitted_model): 
  plt.figure(figsize=(9.8,5), dpi=100) 
  for i, plot_type in enumerate(['Decision Boundary', 'Decision P robabilities']):
    plt.subplot(1,2,i+1)
    mesh_step_size = 0.01 # step size in the mesh 
    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1
    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size ), np.arange(y_min, y_max, mesh_step_size)) 
    if i == 0:
      Z = fitted_model.predict(np.c_[xx.ravel(), yy.ravel()])  
    else:
      try:
        Z = fitted_model.predict_proba(np.c_[xx.ravel(), yy .ravel()])[:,1] 
      except:
        plt.text(0.4, 0.5, 'Probabilities Unavailable', hor izontalalignment='center', 
        verticalalignment='center', transform = pl t.gca().transAxes, fontsize=12) 
        plt.axis('off') 
      break 
    Z = Z.reshape(xx.shape) 
    plt.scatter(X[y.values==0,0], X[y.values==0,1], alpha=0.8, label='YES', s=5, color='navy') 
    plt.scatter(X[y.values==1,0], X[y.values==1,1], alpha=0.8, label='NO', s=5, color='darkorange') 
    plt.imshow(Z, interpolation='nearest', cmap='RdYlBu_r', alp ha=0.15, 
    extent=(x_min, x_max, y_min, y_max), origin='low er') 
    plt.title(plot_type + '\n' + 
    str(fitted_model).split('(')[0]+ ' Test Accuracy: ' + str(np.round(fitted_model.score(X, y), 5))) 
    plt.gca().set_aspect('equal'); 
    plt.tight_layout() 
    plt.legend() 
    plt.subplots_adjust(top=0.9, bottom=0.08, wspace=0.02)

model = LogisticRegression() 
model.fit(X_train1,y_train1) 
plot_bank(X_test1, y_test1, model) 
plt.show()

"""We can see that our data shows the problem caused by imbalanced data. we can barely see the blue points. So we will implement the method on balanced data set.

8.2 Visualize on balanced data
"""

data1 = pd.DataFrame(data=x_resampled) 
X = data1.iloc[:,:-1] 
y = data1.iloc[:,-1] 
pca = PCA(n_components=2).fit_transform(X) 
X_train1, X_test1, y_train1, y_test1 = train_test_split(pca, y, ran dom_state=0) 

plt.figure(dpi=120) 
plt.scatter(pca[y.values==0,0], pca[y.values==0,1], alpha=0.5, labe l='YES', s=2, color='navy') 
plt.scatter(pca[y.values==1,0], pca[y.values==1,1], alpha=0.5, labe l='NO', s=2, color='darkorange') 
plt.legend() 
plt.title('Covid-19 dataset\nFirst Two Principal Components') 
plt.xlabel('PC1') 
plt.ylabel('PC2') 
plt.gca().set_aspect('equal') 
plt.show()

model = LogisticRegression() 
model.fit(X_train1,y_train1) 
plot_bank(X_test1, y_test1, model) 
plt.show()

"""The plot on balanced data is much more better. While the PCA has reduced the accuracy of our Logistic Regression model.This is because we use PCA to reduce the amount of the dimension, which means we removed information from our data and the accuracy become lower.

9. Visualize SVM classifier
"""

import numpy as np 
import csv 
from sklearn import svm 
import matplotlib.pyplot as plt 
from mpl_toolkits.mplot3d import Axes3D 

n_Support_vector = svm_linear.n_support_  
sv_idx = svm_linear.support_  
w = svm_linear.coef_  
b = svm_linear.intercept_ 
X = data.iloc[:,:-1] 
y = data.iloc[:,-1] 
pca2 = PCA(n_components=3).fit_transform(X) 
X_train2, X_test2, y_train2, y_test2 = train_test_split(pca2, y, ra ndom_state=0)  
  
ax = plt.subplot(111, projection='3d') 
x = np.arange(0,1,0.1) 
y = np.arange(0,1,0.1) 
x, y = np.meshgrid(x, y) 
z = (w[0,0]*x + w[0,1]*y + b) / (-w[0,2]) 
surf = ax.plot_surface(x, y, z, rstride=1, cstride=1) 
  
x_array = np.array(X_train2, dtype=float) 
y_array = np.array(y_train2, dtype=int) 
pos = x_array[np.where(y_array==1)] 
neg = x_array[np.where(y_array==-1)] 
ax.scatter(pos[:,0], pos[:,1], pos[:,2], c='r', label='pos') 
ax.scatter(neg[:,0], neg[:,1], neg[:,2], c='b', label='neg')

"""10. Performance of ML algorithms"""

df = pd.DataFrame({"Algorithms": ['Logistic Regression Method 1', ' Logistic Regression Method 1', 'SVM Method'], 
 "Oversampling method": ['SMOTE', 'SMOTE', 'SMOTE '], 
 "Area under ROC curve": [auc_6_3_1, auc_6_3_2, a uc_7_4], 
 "Accuracy": [accuracy_6_3_1, accuracy_6_3_2, acc uracy_7_4], 
 "Precision": [precision_6_3_1, precision_6_3_2, precision_7_4], 
 "Recall": [recall_6_3_1, recall_6_3_2, recall_7_ 4]}) 

df